#!/usr/bin/env python
# coding: utf-8

import re
import os
import pandas as pd
import zipfile
from tqdm.notebook import tqdm as tqdm
import numpy as np


import zipfile
def file2zip(zip_file_name: str, file_names: list):
    """ å°†å¤šä¸ªæ–‡ä»¶å¤¹ä¸­æ–‡ä»¶å‹ç¼©å­˜å‚¨ä¸ºzip
    
    :param zip_file_name:   /root/Document/test.zip
    :param file_names:      ['/root/user/doc/test.txt', ...]
    :return: 
    """
    # è¯»å–å†™å…¥æ–¹å¼ ZipFile requires mode 'r', 'w', 'x', or 'a'
    # å‹ç¼©æ–¹å¼  ZIP_STOREDï¼š å­˜å‚¨ï¼› ZIP_DEFLATEDï¼š å‹ç¼©å­˜å‚¨
    with zipfile.ZipFile(zip_file_name, mode='w', compression=zipfile.ZIP_DEFLATED) as zf:
        for fn in file_names:
            parent_path, name = os.path.split(fn)
            
            # zipfile å†…ç½®æä¾›çš„å°†æ–‡ä»¶å‹ç¼©å­˜å‚¨åœ¨.zipæ–‡ä»¶ä¸­ï¼Œ arcnameå³zipæ–‡ä»¶ä¸­å­˜å…¥æ–‡ä»¶çš„åç§°
            # ç»™äºˆçš„å½’æ¡£åä¸º arcname (é»˜è®¤æƒ…å†µä¸‹å°†ä¸ filename ä¸€è‡´ï¼Œä½†æ˜¯ä¸å¸¦é©±åŠ¨å™¨ç›˜ç¬¦å¹¶ä¼šç§»é™¤å¼€å¤´çš„è·¯å¾„åˆ†éš”ç¬¦)
            zf.write(fn, arcname='results/' + name)
def getSubmit(myname,df):
    print(myname)
    df = df.drop_duplicates()
    write_path1 = writeAns(df,myname+'/results') #å†™å…¥æ–‡ä»¶,å¾—åˆ°write path çš„æ–‡ä»¶å¤¹
    filenames = [write_path1 +'/'+ x for x in os.listdir(write_path1) if '.tag' in x]
    if len(filenames) != 21:
        raise ValueError('æ–‡ä»¶æ²¡æœ‰21ä¸ªï¼')
    file2zip('../user_data/'+myname+'/results'+'.zip',filenames)
def writeAns(r,path_name):
    '''å†™å…¥æäº¤æ–‡ä»¶'''
    r = r[['text_id','from_index','to_index','entity','word']]
    base_path = '../user_data/' + path_name 
    if not os.path.exists(base_path):
        os.makedirs(base_path)
    for i in range(21):
        filename = base_path+'/'+str(i) + '.tag'
        with open(filename,'w+') as f:
            for i,row in r[r.text_id==i].sort_values('from_index').iterrows():
                line = row.tolist()
                line = '#'.join([str(x) for x in line[1:]]) + '\n'
                f.write(line)
    pd.DataFrame(r).to_csv(base_path + '.csv',index=None)
    print(base_path)
    return base_path


# In[3]:


def decorate(word, entity):
    colors = [
        'Gold', 'DarkTurquoise', 'NavajoWhite', 'Orange', 'Blue', 'Brown',
        'Peru', 'GreenYellow', 'red', 'DarkGreen'
    ]
    entitys = [
        'Tsize', 'Caps', 'Sate', 'Tloc', 'This', 'Tnum', 'Tdiff', 'LC', 'MVI',
        'TNM'
    ]
    color_map = {k: v for k, v in zip(entitys, colors)}
    color = color_map[entity]
    return "<font size = '4' color='" + color + "'>" + word + "</font>"


# In[4]:


def to_set(df):
    '''dfè½¬set'''
    df = df[['text_id',  'from_index', 'to_index', 'entity','word']].sort_values(['text_id','from_index'])
    s = []
    for i,line in df.iterrows():
        s.append(tuple(line))
    return set(s)


# In[5]:


# ç”¨äºè¯»å–é¢„å¤„ç†æ–‡æœ¬


# In[6]:


def readText(filename):
    '''è¯»å–æ–‡æœ¬'''
    with open(filename, encoding='gbk') as f:
        content = f.readlines()
    return ''.join([x.replace('\n','ï¼Œï¼Œ') for x in content])
#è¯»å–è®­ç»ƒé›†
def readdata(text_id):
    with open('../raw_data/sample_example/%s.txt' % (text_id),encoding='gbk') as f:
        content = f.readlines()
    content = ''.join([x.replace('\n','ï¼Œï¼Œ') for x in content])
    ann_info = pd.read_csv('../raw_data/sample_example/%s.tag' % (text_id),header=None)
    ann_info = ann_info[0].str.split('#',expand=True)
    ann_info.columns = [ 'from_index', 'to_index','entity','word']
    ann_info['text_id'] = text_id
    ann_info = ann_info[['text_id', 'entity', 'from_index', 'to_index', 'word']]
    ann_info = ann_info.astype({'text_id':'int','entity':'str','from_index':'int','to_index':'int','word':'str'})
    return content,ann_info
def writeTrainfile(i,text,ann_info):
    text_file = train_data_path + 'data/' + str(i) + '.txt'
    ann_file =  train_data_path + 'label/' + str(i) +'.csv' 
    with open(text_file,'w+',encoding='utf-8') as f:
        f.write(text)
    ann_info.to_csv(ann_file,index=None)


# In[7]:


def readAnn(filename,text_id):
    '''è¯»å–çš„å•ä¸ªæ–‡ä»¶ä¿¡æ¯'''
    ann_info = pd.read_csv(filename,header=None,engine='python')
#     print(ann_info)
    ann_info = ann_info[0].str.split('#',expand=True)
    ann_info.columns = [ 'from_index', 'to_index','entity','word']
    ann_info['text_id'] = text_id
    ann_info = ann_info[['text_id', 'entity', 'from_index', 'to_index', 'word']]
    ann_info = ann_info.astype({'text_id':'int','entity':'str','from_index':'int','to_index':'int','word':'str'})
    return ann_info
def readResults(name):
    '''ä»æ–‡ä»¶å¤¹è¯»å–æ ‡æ³¨ä¿¡æ¯ï¼Œè¿”å›Dataframe'''
    res_path = '../user_data/' + name
    ann = []
    for file_name in os.listdir(res_path):
        if file_name=='.ipynb_checkpoints':
            continue
        text_id = file_name.split('.')[0]
        file_name = res_path + '/'+ file_name
        ann.append(readAnn(file_name,text_id))
    #     break
    return pd.concat(ann)


# # partten

# In[8]:


def findPatten(P,PATTENS,texts):
    res = [x for x in PATTENS[P].finditer(texts)]
    return res  #
corpus = 'å°æ¢å‹ã€ç»å…¸å‹ã€å‡è…ºæ ·å‹ã€ç´«ç™œå‹ã€ç´«ç™œæ ·å‹ã€å·¨ç»†èƒå‹ã€ç ´éª¨ç»†èƒå·¨ç»†èƒå‹ã€æ¢­å½¢ç»†èƒå‹ã€å¤šå½¢ç»†èƒå‹ã€é€æ˜ç»†èƒå‹ã€å¯Œå«è„‚è´¨å‹ã€ç¡¬åŒ–å‹ã€ç±»é»‘è‰²ç´ å‹ã€æ·‹å·´é—´è´¨å‹ã€å°ç»†èƒå‹ã€ä½åˆ†åŒ–å‹ã€æœªåˆ†åŒ–å‹ã€è‡ªå‘åæ­»å‹ã€å‡è…ºç®¡å‹ã€å¯Œè„‚è´¨å‹ã€å‡è…ºæ ·å‹ã€å¯Œäºè„‚è´¨å‹ã€æ¢ç´¢å‹ã€ç²—æ¢å‹ã€å¯Œè„‚å‹ã€ç¡¬åŒ–å‹ã€ç²—æ¢å‹ã€å®ä½“å‹ã€å‡è…ºå‹ã€ç»†æ¢å‹ã€é€æ˜ç»†èƒå‹ã€é€æ˜ç»†èƒäºšå‹ã€èŠå½¢å›¢å‹ã€æ³¡æ²«æ ·ç»†èƒ'
This_dict = corpus.replace('ã€','|')
PATTENS_KUOHAO = {
'Tloc-å·¦ä¸­å³è‚' : re.compile(r'([å·¦ä¸­å³][å·¦ä¸­å³åŠã€ä¸‰]*è‚)(è‚¿ç˜¤|ç™Œ|è‚¿ç‰©|ç»„ç»‡|ã€|ï¼‰)'),
'Tloc-å°¾çŠ¶å¶'  : re.compile(r'è‚*(å°¾çŠ¶å¶)(è‚¿ç˜¤|ç™Œ|è‚¿ç‰©|ç»„ç»‡|ï¼‰)'), 
'Tloc-å·¦å³åŠè‚' : re.compile(r'([å·¦ä¸­å³]åŠè‚)'),
'Tloc-è‚å‰åå¤–å¶' : re.compile(r'([å·¦ä¸­å³][è‚å‰åå†…å¤–]*å¶)')
}
PATTENS = {

'MVI-1' : re.compile('[^x](M\dçº§*)'), 
'MVI-3' : re.compile('([æŸ¥æœªå¯]*è§)[^è‚‰]?[^çœ¼]?è„‰ç®¡å†…*ç™Œæ “[^\(ï¼ˆ]'),
'MVI-4' : re.compile('([æŸ¥æœªå¯]*è§).?.?å¾®è¡€ç®¡å†…*ç™Œæ “'),
'MVI-5' : re.compile('([æŸ¥æœªå¯]*è§).?.?å¾®è¡€ç®¡å†…*ä¾µçŠ¯'),
    
'TNM-1' : re.compile(r'(\w*?T\({0,1}\w\){0,1}\w*?M[x\d])'), #
  
'Tnum-1' : re.compile(r'è‚¿[ç˜¤ç‰©]å…±*(.ä¸ª)') , 
'Tnum-2' : re.compile(r'è‚¿ç˜¤ç»“èŠ‚(\dæš)'),   
'Tnum-3' : re.compile(r'(\dæš)è‚¿ç˜¤ç»“èŠ‚'),  

'Tsize-1' :  re.compile(r'[^äº><](\d[\.\-Ã—~\d \*]*cm)'), 

'Tloc-4' : re.compile(r'(?<=æ®µã€)([â…£â…¡â…¤â…¦â…§â…¥IV]+æ®µ)'), 
'Tloc-5' : re.compile(r'[^ã€]([â…£â…¡â…¤â…¦â…§â…¥\-ã€IV]+æ®µ)'),
'Tloc-6' : re.compile(r'([â…£â…¡â…¤â…¦â…§â…¥\-ã€IV])è‚¿ç˜¤'),

# s6æ®µã€s8æ®µ è”åˆ     
'Tloc-sxã€sxã€sx' : re.compile(r'[è‚ç™Œç‰©ç˜¤]([Ss][1-9ã€Ss\-\.]+\d)'),
'Tloc-sx' : re.compile(r'[è‚ç™Œç‰©ç˜¤]([Ss]\d)[^\.\-][^S\d]'), # è¿‡æ»¤S100
'Tloc-9' : re.compile(r'[^ã€]([I-V-ã€]+æ®µ)'), #è‚II-IIIæ®µ VIIæ®µ  è‚IVã€Væ®µ
  

'LC-æ—©æœŸè‚ç¡¬åŒ–' : re.compile(r'(æ—©æœŸ)è‚ç¡¬åŒ–'),  # æ—©æœŸè‚ç¡¬åŒ–ã€‚æ¢è¡Œ çš„æƒ…å†µ2ä¸ªã€‚æœªè€ƒè™‘åˆ°
'LC-é™æ­¢æœŸ' : re.compile(r'(é™æ­¢æœŸ)'),
'LC-çº¤ç»´åŒ–åˆ†æœŸSï¼ˆ0~4æœŸï¼‰ï¼š\dæœŸ' : re.compile(r'çº¤ç»´åŒ–åˆ†æœŸSï¼ˆ0~4æœŸï¼‰ï¼š([\d\-~]{1,3}æœŸ)'), 
'LC-G1s1' : re.compile(r'[ï¼Œ,ï¼ˆ\(](G\dS[\d\w\-]*)[ï¼Œ,ï¼‰\)]') , 
'LC-çº¤ç»´åŒ–næœŸ' : re.compile(r'(è‚*çº¤ç»´åŒ–*[\d\-]{1,4}æœŸ)'), 

'LC-æ´»åŠ¨æœŸ' : re.compile(r'(æ´»åŠ¨æœŸ)'),
'LC-å¤§å°ç»“èŠ‚æ€§' : re.compile(r'å‘ˆ(å¤§å°ç»“èŠ‚æ··åˆæ€§)è‚ç¡¬åŒ–[^æ”¹å˜]{2}'),

'This-all' : re.compile(r'('+This_dict+')'),
    
'Tdiff-xçº§' : re.compile(r'([IVâ… â…¡â…¢â…£â…¤â…¥â…¦â…§â…¨â…©IV\-~]+çº§)'),
'Tdiff-ä½åˆ†åŒ–ç›¸å…³' : re.compile(r'[^-](ä½åˆ†åŒ–)[èƒ†]'),
'Tdiff-ä¸­åˆ†åŒ–ç›¸å…³' : re.compile(r'[^å…¶](ä¸­åˆ†åŒ–)[èƒ†]'),
'Tdiff-é«˜åˆ†åŒ–ç›¸å…³' : re.compile(r'[^è€ƒ]?[^è™‘](é«˜åˆ†åŒ–)'),
'Tdiff-ä¸­_ä½åˆ†åŒ–ç›¸å…³' : re.compile(r'([é«˜ä¸­ä½]-[é«˜ä¸­ä½]åˆ†åŒ–)[èƒ†]'),
'Tdiff-ä¸­ä½2åˆ†åŒ–ç›¸å…³' : re.compile(r'[^å…¶](ä¸­ä½åˆ†åŒ–)'),

# åŒ…è†œ æ€»æ•°246
'Caps-åŒ…è†œå®Œæ•´ä¸å®Œæ•´' : re.compile(r'åŒ…è†œå°š*?([ä¸]*?å®Œæ•´)'), #7
# # 'Caps-æœªçªç ´åŒ…è†œ' : re.compile(r'(æœªçªç ´)åŒ…è†œ'), # 2 AB ğŸ–
'Caps-æœªç©¿ç ´åŒ…è†œ' : re.compile(r'(ä¸å®Œæ•´|å®Œæ•´|æœªç©¿ç ´|æœªç©¿é€|ä¾µåŠ)åŒ…è†œ'), # 3 AB
'Caps-æœªè§å¯è§è‚¿ç˜¤åŒ…è†œ' : re.compile(r'(æœ‰|æ— |æœªè§|å¯è§)æ˜*æ˜¾*è‚¿*ç˜¤*åŒ…è†œ') ,#11 ğŸ‘†
'Caps-ä¾µçŠ¯åŒ…è†œ' : re.compile(r'(ä¾µçŠ¯)è‚¿*ç˜¤*åŒ…è†œ'), #75 ğŸ‘†
'Caps-åŒ…è†œä¾µçŠ¯' : re.compile(r'åŒ…è†œä¼´*(ä¾µçŠ¯)'), #3 A


'Sate-xxå«æ˜Ÿå­ç¶!å½¢æˆ' : re.compile(r'(å¯*è§|æœªè§|æŸ¥è§|å¤šå‘)æ˜*[ç¡®æ˜¾]*å«æ˜Ÿ.ç¶(?!å½¢æˆ)'), #xxå«æ˜Ÿå­ç¶,åé¢æ²¡æœ‰å½¢æˆ 
'Sate-æœªè§å«æ˜Ÿå­ç¶å½¢æˆ' : re.compile(r'(æœªè§)æ˜*[ç¡®æ˜¾]*å«æ˜Ÿ.ç¶å½¢æˆ'), # æœªè§å’Œå½¢æˆï¼Œå°†æœªè§æ ‡å‡º 
'Sate-è§å«æ˜Ÿå­ç¶å½¢æˆ' : re.compile(r'[^æœªè§æ˜ç¡®æ˜¾]å«æ˜Ÿ.ç¶(å½¢æˆ)'), #
'Sate-å¯è§æ˜Ÿå­ç¶å½¢æˆ' : re.compile(r'å¯è§å«æ˜Ÿ.ç¶(å½¢æˆ)'),#
'Sate-è§å«æ˜Ÿå­ç¶ï¼ˆï¼‰å½¢æˆ' : re.compile(r'å«æ˜Ÿ.ç¶ï¼ˆ.*ï¼‰(å½¢æˆ)'),  # 
'Sate-ä¼´æŸ¥è§é•œä¸‹å«æ˜Ÿå­ç¶' : re.compile(r'ä¼´(æŸ¥è§)é•œä¸‹å«æ˜Ÿ.ç¶'),  #
'Sate-æœªè§.*åŠå«æ˜Ÿ.ç¶' : re.compile(r'(æœªè§).{0,20}?[åŠå’Œ][^æœªè§]{0,2}?å«æ˜Ÿ.ç¶'),#
'Sate-[æŸ¥å¯]è§.*åŠå«æ˜Ÿ.ç¶' : re.compile(r'([æŸ¥å¯]è§).{0,20}[åŠå’Œ][^æŸ¥å¯è§å¤šå‘]{0,2}å«æ˜Ÿ.ç¶[^å½¢æˆ]{12,20}'),#

'Sate-ä¼´.*åŠå«æ˜Ÿ.ç¶' : re.compile(r'ä¼´*.{0,10}å«æ˜Ÿ.ç¶[åŠå’Œ][^æŸ¥å¯è§å¤šå‘].{0,15}(å½¢æˆ)'), #
'Sate-å«æ˜Ÿå­ç¶åŠèƒ†ç®¡å†…ç™Œæ “(å½¢æˆ)' : re.compile(r'å«æ˜Ÿå­ç¶åŠèƒ†ç®¡å†…ç™Œæ “(å½¢æˆ)'), 

}  


# # extract

# In[9]:


def filtLC(df,p,flag):
    '''filt partten'''
    lc = df
    length = len(df)
    if flag:
        lc = [x for x in lc if not re.match(p,x[4])]
    return lc,length - len(lc)
def dealLC2(df,p1,p2,entity):
    '''ä¿ç•™l1'''
    keep = [x for x in df if x[3]!=entity]
    lc = [x for x in df if x[3]==entity]
    length = len(df)
    four = False
    huodongqi = False
    #å¯»æ‰¾l1
    for i in lc:
        if re.match(p1,i[4]) :
            four = True

            break
    #å¯»æ‰¾l2
    for i in lc:
        if re.match(p2,i[4]) :
            huodongqi = True
            break
    if huodongqi and four :
        lc = [x for x in lc if not re.match(p2,x[4])]
    res = keep + lc
    if len(res) != len(df):
#         print(p1)
#         print('del',set(df) - set(res))
        pass
    return res,length - len(res)
def getAns(texts,ids,split_off,pattens):
    '''æŠ½å–groupé‡Œçš„å®ä½“'''
    df = []
    #æŠ½å–é€»è¾‘
    for P in pattens:
        res = findPatten(P,pattens,texts)
#         if res!=[]:
#             print(res)
        for line in res:
            if len(line.group()) < 1:
                continue
            b = int(line.span(1)[0])
            e = int(line.span(1)[1])
            c = P.split('-')[0]
            word = texts[b:e]
            if  len(word) < 1:
                continue
            df.append((ids, b + split_off, e + split_off, c, word))   
    return df
def filt2(df,entity):
    '''åå¤„ç†ï¼Œç»“èŠ‚æ€§ä¼˜å…ˆçº§æœ€ä½'''
    keep = [x for x in df if x[3]!=entity]
    lc = [x for x in df if x[3]==entity]
    no_jiejie = [x for x in lc if 'ç»“èŠ‚' not in x[4]] 
    jiejie = [x for x in lc if 'ç»“èŠ‚' in x[4]]
#     print(no_jiejie, len(no_jiejie))
    if len(no_jiejie) != 0: #å¦‚æœå­˜åœ¨å…¶ä»–çš„L
        return keep + no_jiejie
    else:
#         print('no LC append jiejie',jiejie)
        return keep + jiejie
    
def extract(texts, ids,text_id,spilt_off=0,filter_flag = False):
    '''æŠ½å–é€»è¾‘'''
    df = getAns(texts,ids,split_off,PATTENS)
    l1 = len(df)
    df = list(set(df)) #å»é‡ 
    l2 = len(df)
    if l1 != l2:
#         print('å»é‡ï¼'+'*' *50)
        pass
    extract_res = df
    if len(df) != 0:
        drop_cnt = 0
        cnt = 0
        # 0.2612 - 0.2692
        #LC åå¤„ç†

        df,cnt = dealLC2(df,re.compile(r'\dæœŸ'),re.compile(r'è‚*çº¤ç»´åŒ–\dæœŸ'),'LC') #     
        df,cnt = dealLC2(df,re.compile(r'\dæœŸ'),re.compile(r'G\dS\d'),'LC') #         
        df,cnt = dealLC2(df,re.compile(r'\d-\dæœŸ'),re.compile(r'è‚*çº¤ç»´åŒ–\d-\dæœŸ'),'LC') # 
        df,cnt = dealLC2(df,re.compile(r'æ—©æœŸ'),re.compile(r'[^åŒ–]*[\d\-\d]{1,3}æœŸ'),'LC') # 
        df,cnt = dealLC2(df,re.compile(r'G\dS[\d\w\-]*'),re.compile(r'æ—©æœŸ'),'LC') # 
        df,cnt = filtLC(df,re.compile(r'[^åŒ–ç»´]*\dæœŸ'),filter_flag) #
        df     = filt2(df,'LC')
        df,cnt = dealLC2(df,re.compile(r'æ´»åŠ¨æœŸ'),re.compile(r'[^åŒ–]*\dæœŸ'),'LC') # 
        df,cnt = dealLC2(df,re.compile(r'å½¢æˆ'),re.compile(r'(è§)'),'Sate') # 
        drop_cnt += cnt
        df,cnt = dealLC2(df,re.compile(r'æœªè§'),re.compile(r'å½¢æˆ'),'Sate') # 
        drop_cnt += cnt
        if drop_cnt>0:
#             print(ids,text_id,extract_res,drop_cnt,df)
            pass
#         if len(df) > 1 and 'ã€è¯Šæ–­ï¼š' in texts:
        if len(df) > 1 :
#             print(df,ids,text_id)
            pass
        
        df = pd.DataFrame(df,columns=['text_id',  'from_index', 'to_index', 'entity','word'])
        t = df.entity.value_counts()
    else:
        df = pd.DataFrame()

    return df

def extract_kuohao(text,i,j,split_off):
    '''æŠ½å–æ‹¬å·é‡Œçš„å†…å®¹åï¼Œå†å¯¹æ–‡æœ¬è¿›è¡Œå¤„ç†'''
    df = []
    ids = i
    kuohao_partten = re.compile(r'[ï¼ˆ\(][^ï¼ˆ]{2,20}[\)ï¼‰]')
    kuohao_content = list(re.finditer(kuohao_partten,text))
    for line in kuohao_content:
        text = line.group()
        start_index = line.span()[0] + split_off
        df += getAns(text,ids,start_index,PATTENS_KUOHAO)
    if len(df) !=0:
        df = pd.DataFrame(df,columns=['text_id',  'from_index', 'to_index', 'entity','word'])
    else:
        df = pd.DataFrame()
    return df


# # run

# In[10]:


allres = []
df = []
data_path =  '../raw_data/train/'
filenamepair = {
        data_path + 'txt/' + str(i) + '.txt':
        data_path + 'label/' + str(i) + '.csv'
        for i in range(20)
    }
all_texts = ''
filter_p = re.compile('G\d-\dS\d')
for i, (text_file_name, ann_file_name) in enumerate(filenamepair.items()):  #è¯»å–æ‰€æœ‰æ–‡æœ¬æ•°æ®
    filter_flag = True
    split_off = 0
    texts = readText(text_file_name)
    all_texts += texts
    if re.search(filter_p,texts): #åŒ¹é…åˆ°æ¨¡å¼ï¼Œä¸è¿›è¡Œè¿‡æ»¤
#         print(re.search(filter_p,texts))
        filter_flag = False
    html = ''
    from_index = 0
    for j,text in enumerate(texts.split('ï¼Œï¼Œï¼Œï¼Œ')):
#         print(text)
        t = []
        t.append(extract(text,i,j,split_off,filter_flag))
        t.append(extract_kuohao(text,i,j,split_off))
        t = pd.concat(t)
        if t.shape[0]!=0:
            t = t.sort_values('from_index').drop_duplicates()
            df.append(t)
            for n, row in t.iterrows():
                html += texts[from_index:row.from_index]  #æ·»åŠ å‰é¢çš„éƒ¨åˆ†
                html += decorate(row.word, row.entity)  #æ·»åŠ å®ä½“éƒ¨åˆ†
                from_index = row.to_index  #æ›´æ–°èµ·ç‚¹
        split_off += len(text) + 4
    html = html.replace('ï¼Œï¼Œ', '<br/>')
    html_name = '../user_data/html/train_' + str(i) + '.html'
    with open(html_name, 'w+', encoding='gbk') as f:
        f.write(html)
df = pd.concat(df).drop_duplicates()
print('æ•°æ®æŠ½å–å®Œæ¯•ï¼',df.shape)
getSubmit('train_data_tag',df)


allres = []
df = []
data_path =  '../raw_data/test/'
filenamepair = {
        data_path + 'txt/' + str(i) + '.txt':
        data_path + 'label/' + str(i) + '.csv'
        for i in range(21)
    }
all_texts = ''
filter_p = re.compile('G\d-\dS\d')
for i, (text_file_name, ann_file_name) in enumerate(filenamepair.items()):  #è¯»å–æ‰€æœ‰æ–‡æœ¬æ•°æ®
    filter_flag = True
    split_off = 0
    texts = readText(text_file_name)
    all_texts += texts
    if re.search(filter_p,texts): #åŒ¹é…åˆ°æ¨¡å¼ï¼Œä¸è¿›è¡Œè¿‡æ»¤
#         print(re.search(filter_p,texts))
        filter_flag = False
    html = ''
    from_index = 0
    for j,text in enumerate(texts.split('ï¼Œï¼Œï¼Œï¼Œ')):
#         print(text)
        t = []
        t.append(extract(text,i,j,split_off,filter_flag))
        t.append(extract_kuohao(text,i,j,split_off))
        t = pd.concat(t)
        if t.shape[0]!=0:
            t = t.sort_values('from_index').drop_duplicates()
            df.append(t)
            for n, row in t.iterrows():
                html += texts[from_index:row.from_index]  #æ·»åŠ å‰é¢çš„éƒ¨åˆ†
                html += decorate(row.word, row.entity)  #æ·»åŠ å®ä½“éƒ¨åˆ†
                from_index = row.to_index  #æ›´æ–°èµ·ç‚¹
        split_off += len(text) + 4
    html = html.replace('ï¼Œï¼Œ', '<br/>')
    html_name = '../user_data/html/train_' + str(i) + '.html'
    with open(html_name, 'w+', encoding='gbk') as f:
        f.write(html)

df = pd.concat(df).drop_duplicates()
print(df.shape)
getSubmit('8989_B',df)

